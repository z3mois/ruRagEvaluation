{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18361cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def has_sentences(example):\n",
    "    \"\"\"\n",
    "    Функция возвращает True, если пример содержит хотя бы одно предложение\n",
    "    в documents_sentences (и, опционально, в response_sentences).\n",
    "    Иначе возвращает False.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"documents_sentences\" not in example:\n",
    "        return False\n",
    "    if not isinstance(example[\"documents_sentences\"], list):\n",
    "        return False\n",
    "    total_doc_sentences = 0\n",
    "    for doc in example[\"documents_sentences\"]:\n",
    "        if isinstance(doc, list):\n",
    "            total_doc_sentences += len(doc)\n",
    "    if total_doc_sentences == 0:\n",
    "        return False\n",
    "\n",
    "    if \"response_sentences\" in example:\n",
    "        if not isinstance(example[\"response_sentences\"], list):\n",
    "            return False\n",
    "        if len(example[\"response_sentences\"]) == 0:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "MODEL_NAME = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def preprocess(example, max_length=1024):\n",
    "    question_ids = tokenizer.encode(example[\"question\"], add_special_tokens=False)\n",
    "    doc_ids, rel_labels, util_labels = [], [], []\n",
    "\n",
    "    for doc in example[\"documents_sentences\"]:\n",
    "        for key, sent in doc:\n",
    "            tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
    "            doc_ids += tokens\n",
    "            rel_labels += [float(key in example[\"all_relevant_sentence_keys\"])] * len(tokens)\n",
    "            util_labels += [float(key in example[\"all_utilized_sentence_keys\"])] * len(tokens)\n",
    "\n",
    "    response_ids = tokenizer.encode(example[\"response\"], add_special_tokens=False)\n",
    "    adh_labels = [float(example[\"adherence_score\"])] * len(response_ids)\n",
    "\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    input_ids = question_ids + [sep_id] + doc_ids + [sep_id] + response_ids\n",
    "\n",
    "    context_mask = [0] * (len(question_ids)+1) + [1]*len(doc_ids) + [0] + [0]*len(response_ids)\n",
    "    response_mask = [0]*(len(question_ids)+len(doc_ids)+2) + [1]*len(response_ids)\n",
    "\n",
    "    rel_labels = [0.0]*(len(question_ids)+1) + rel_labels + [0.0]*(len(response_ids)+1)\n",
    "    util_labels = [0.0]*(len(question_ids)+1) + util_labels + [0.0]*(len(response_ids)+1)\n",
    "    adh_labels = [0.0]*(len(question_ids)+len(doc_ids)+2) + adh_labels\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        context_mask = context_mask[:max_length]\n",
    "        response_mask = response_mask[:max_length]\n",
    "        rel_labels = rel_labels[:max_length]\n",
    "        util_labels = util_labels[:max_length]\n",
    "        adh_labels = adh_labels[:max_length]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([1]*len(input_ids), dtype=torch.long),\n",
    "        'context_mask': torch.tensor(context_mask, dtype=torch.bool),\n",
    "        'response_mask': torch.tensor(response_mask, dtype=torch.bool),\n",
    "        'labels_relevance': torch.tensor(rel_labels, dtype=torch.float),\n",
    "        'labels_utilization': torch.tensor(util_labels, dtype=torch.float),\n",
    "        'labels_adherence': torch.tensor(adh_labels, dtype=torch.float),\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbfcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promt_test.data import load_full_ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1db901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_full_ragbench()#load_dataset(\"rungalileo/ragbench\", \"delucionqa\")\n",
    "dataset = {key:dataset[key].filter(has_sentences) for key in dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82213710",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = tokenizer.model_max_length  # максимально допустимая длина входа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(example):\n",
    "    question_ids = tokenizer.encode(example[\"question\"], add_special_tokens=False)\n",
    "    doc_ids = []\n",
    "    for doc in example[\"documents_sentences\"]:\n",
    "        for key, sent in doc:\n",
    "            doc_ids.extend(tokenizer.encode(sent, add_special_tokens=False))\n",
    "    response_ids = tokenizer.encode(example[\"response\"], add_special_tokens=False)\n",
    "    total = len(question_ids) + 1 + len(doc_ids) + 1 + len(response_ids)\n",
    "    return total\n",
    "\n",
    "def filter_by_length(example):\n",
    "    return count_tokens(example) <= MAX_LEN\n",
    "\n",
    "\n",
    "filtered = {\n",
    "    ds_name: ds_dict.filter(filter_by_length)\n",
    "    for ds_name, ds_dict in dataset.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d18724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_splits = [ds[\"train\"] for ds in filtered.values() if \"train\" in ds]\n",
    "val_splits   = [ds[\"validation\"] for ds in filtered.values() if \"validation\" in ds]\n",
    "test_splits  = [ds[\"test\"] for ds in filtered.values() if \"test\" in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "combined = DatasetDict({\n",
    "    \"train\":      concatenate_datasets(train_splits),\n",
    "    \"validation\": concatenate_datasets(val_splits),\n",
    "    \"test\":       concatenate_datasets(test_splits)\n",
    "})\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a020d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     # batch: list of dicts\n",
    "#     keys = batch[0].keys()\n",
    "#     out = {}\n",
    "#     #  max len\n",
    "#     max_len = MAX_LEN #max(len(x['input_ids']) for x in batch)\n",
    "# #     print(batch)\n",
    "#     for k in keys:\n",
    "#         vals = [x[k] for x in batch]\n",
    "#         if k == 'input_ids':\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "#         elif k == 'attention_mask':\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=0)\n",
    "#         elif k in ['context_mask', 'response_mask']:\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=False)\n",
    "#         elif k.startswith('labels_'):\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=0)\n",
    "#         else:\n",
    "#             out[k] = torch.stack(vals)\n",
    "#     return out\n",
    "# train_data = [preprocess(ex) for ex in combined['train']]\n",
    "# val_data   = [preprocess(ex) for ex in combined['validation']]\n",
    "# batch_size=8\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1abb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(x: torch.Tensor, max_len: int, pad_value: int):\n",
    "    L = x.size(0)\n",
    "    if L < max_len:\n",
    "        # дополняем в конец\n",
    "        pad = x.new_full((max_len - L,), pad_value)\n",
    "        return torch.cat([x, pad], dim=0)\n",
    "    else:\n",
    "        # усекаем\n",
    "        return x[:max_len]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    max_len = MAX_LEN\n",
    "\n",
    "    for k in keys:\n",
    "        vals = [x[k] for x in batch]\n",
    "        if k == 'input_ids':\n",
    "            # сначала усекаем/падим каждый, потом стэкаем\n",
    "            processed = [pad_or_truncate(x, max_len, tokenizer.pad_token_id) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        elif k == 'attention_mask':\n",
    "            processed = [pad_or_truncate(x, max_len, 0) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        elif k in ['context_mask', 'response_mask']:\n",
    "            processed = [pad_or_truncate(x.to(torch.uint8), max_len, 0).to(torch.bool) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        elif k.startswith('labels_'):\n",
    "            processed = [pad_or_truncate(x, max_len, 0) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        else:\n",
    "            out[k] = torch.stack(vals, dim=0)\n",
    "    return out\n",
    "train_data = [preprocess(ex) for ex in combined['train']]\n",
    "val_data   = [preprocess(ex) for ex in combined['validation']]\n",
    "batch_size=8\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['train'][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ec9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ed8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(combined['validation']['relevance_score']), np.mean(combined['validation']['utilization_score']), np.mean(combined['validation']['completeness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = preprocess(combined['train'][1])\n",
    "\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(processed[\"input_ids\"])\n",
    "\n",
    "labels_rel = processed[\"labels_relevance\"]\n",
    "labels_util = processed[\"labels_utilization\"]\n",
    "labels_adh = processed[\"labels_adherence\"]\n",
    "\n",
    "print(f\"{'Token':20} | {'Relevance':9} | {'Utilization':11} | {'Adherence':9}\")\n",
    "print(\"-\" * 60)\n",
    "i = 0\n",
    "for token, rel, util, adh in zip(tokens, labels_rel, labels_util, labels_adh):\n",
    "    if token == '[PAD]':\n",
    "        break\n",
    "    i+=1\n",
    "    print(f\"{token:20} | {rel:9.1f} | {util:11.1f} | {adh:9.1f}\")\n",
    "    if token == '▁Simply':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "class DebertaTrace(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hid = base_model.config.hidden_size\n",
    "        self.rel_head = nn.Linear(hid,1)\n",
    "        self.util_head = nn.Linear(hid,1)\n",
    "        self.adh_head = nn.Linear(hid,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hs = out.last_hidden_state\n",
    "        return {\n",
    "            'logits_relevance': self.rel_head(hs).squeeze(-1),\n",
    "            'logits_utilization': self.util_head(hs).squeeze(-1),\n",
    "            'logits_adherence': self.adh_head(hs).squeeze(-1)\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_batch_metrics(logits, labels, masks):\n",
    "    # logits: dict of tensors [B,L]\n",
    "    # labels: dict of tensors [B,L]\n",
    "    # masks: dict of bool tensors [B,L]\n",
    "    rel_log, util_log, adh_log = logits['logits_relevance'], logits['logits_utilization'], logits['logits_adherence']\n",
    "    rel_lab, util_lab, adh_lab = labels['labels_relevance'], labels['labels_utilization'], labels['labels_adherence']\n",
    "    ctx_m, resp_m = masks['context_mask'], masks['response_mask']\n",
    "\n",
    "    # probs\n",
    "    rel_p = torch.tensor(expit(rel_log.detach().cpu().numpy()))\n",
    "    util_p = torch.tensor(expit(util_log.detach().cpu().numpy()))\n",
    "    adh_p = torch.tensor(expit(adh_log.detach().cpu().numpy()))\n",
    "    # preds\n",
    "    pred_rel = rel_p>0.5\n",
    "    pred_util = util_p>0.5\n",
    "    pred_adh = adh_p>0.5\n",
    "    # true\n",
    "    true_rel = rel_lab.detach().cpu()==1\n",
    "    true_util = util_lab.detach().cpu()==1\n",
    "    true_adh = adh_lab.detach().cpu()==1\n",
    "\n",
    "    # per example\n",
    "    def ex_metric(pred, true, mask):\n",
    "        inter = (pred & true & mask).sum(dim=1).float()\n",
    "        denom = (true & mask).sum(dim=1).float()\n",
    "        return (inter/denom.clamp(min=1)).mean().item()\n",
    "\n",
    "    rel_m = ex_metric(pred_rel, true_rel, ctx_m)\n",
    "    util_m= ex_metric(pred_util, true_util, ctx_m)\n",
    "    adh_m = ex_metric(pred_adh, true_adh, resp_m)\n",
    "    comp = ((pred_rel & pred_util & ctx_m).sum(dim=1).float()/ (true_rel & ctx_m).sum(dim=1).float().clamp(min=1)).mean().item()\n",
    "    return rel_m, util_m, adh_m, comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(logits, labels, mask, head_name=\"\"):\n",
    "    \"\"\"\n",
    "    Проверяет тензоры logits и labels для данного сегмента mask на наличие NaN.\n",
    "    Если NaN найдены, выводит подробную информацию и выбрасывает ошибку.\n",
    "    \"\"\"\n",
    "    # Извлечение элементов по маске\n",
    "    logit_vals = logits[mask]\n",
    "    label_vals = labels[mask]\n",
    "    \n",
    "    # Проверка на NaN\n",
    "    nan_logits = torch.isnan(logit_vals)\n",
    "    nan_labels = torch.isnan(label_vals)\n",
    "    \n",
    "    if nan_logits.any() or nan_labels.any():\n",
    "        print(f\"NaN detected in head '{head_name}':\")\n",
    "        if nan_logits.any():\n",
    "            idxs = torch.nonzero(nan_logits, as_tuple=True)[0]\n",
    "            print(f\"  logits NaN at positions: {idxs.tolist()}\")\n",
    "            print(f\"  sample logits: {logit_vals[idxs].tolist()}\")\n",
    "        if nan_labels.any():\n",
    "            idxs = torch.nonzero(nan_labels, as_tuple=True)[0]\n",
    "            print(f\"  labels NaN at positions: {idxs.tolist()}\")\n",
    "            print(f\"  sample labels: {label_vals[idxs].tolist()}\")\n",
    "        # Остановим обучение для отладки\n",
    "        raise ValueError(f\"NaN encountered in head '{head_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trace_metrics_inference(logits, masks, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Считает для каждого примера батча:\n",
    "      - relevance_rate:   доля токенов контекста, предсказанных как релевантные\n",
    "      - utilization_rate: доля токенов контекста, предсказанных как используемые\n",
    "      - adherence_rate:   доля токенов ответа,   предсказанных как подкреплённые (adherent)\n",
    "      - completeness:     доля релевантных предсказаний, которые ещё и используются:\n",
    "                          |pred_rel ∧ pred_util| / |pred_rel|\n",
    "    Параметры:\n",
    "      logits – словарь из трёх [B,L] тензоров: logits_relevance,\n",
    "               logits_utilization, logits_adherence\n",
    "      masks  – словарь из двух [B,L] булевых тензоров: context_mask, response_mask\n",
    "      threshold – граница для сигмоида, default=0.5\n",
    "    Возвращает словарь из четырёх [B] тензоров с метриками по примерам.\n",
    "    \"\"\"\n",
    "    rel_pred  = (torch.sigmoid(logits['logits_relevance'].detach().cpu())  > threshold)\n",
    "    util_pred = (torch.sigmoid(logits['logits_utilization'].detach().cpu())> threshold)\n",
    "    adh_pred  = (torch.sigmoid(logits['logits_adherence'].detach().cpu())   > threshold)\n",
    "\n",
    "    ctx_m  = masks['context_mask'].detach().cpu()\n",
    "    resp_m = masks['response_mask'].detach().cpu()\n",
    "\n",
    "    def rate(pred, mask):\n",
    "        # sum(pred & mask) / sum(mask)\n",
    "        num = (pred & mask).sum(dim=1).float()\n",
    "        den = mask.sum(dim=1).float().clamp(min=1)\n",
    "        return num.div(den)\n",
    "\n",
    "    relevance_rate   = rate(rel_pred,  ctx_m)\n",
    "    utilization_rate = rate(util_pred, ctx_m)\n",
    "    adherence_rate   = rate(adh_pred,  resp_m)\n",
    "\n",
    "    # completeness: из релевантных предсказаний — сколько ещё и util\n",
    "    num_ru = (rel_pred & util_pred & ctx_m).sum(dim=1).float()\n",
    "    den_r  = rel_pred.sum(dim=1).float().clamp(min=1)\n",
    "    completeness = num_ru.div(den_r)\n",
    "\n",
    "    return {\n",
    "        'relevance_rate':   relevance_rate,    \n",
    "        'utilization_rate': utilization_rate,  \n",
    "        'adherence_rate':   adherence_rate,\n",
    "        'completeness':     completeness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(logits, labels, masks):\n",
    "    \"\"\"\n",
    "    Считает precision/recall/F1 по каждому TRACE-лейблу на всём батче.\n",
    "    Возвращает словарь из девяти скалярных метрик.\n",
    "    \"\"\"\n",
    "    rel_pred  = (torch.sigmoid(logits['logits_relevance'].detach().cpu())  > 0.5) & masks['context_mask']\n",
    "    util_pred = (torch.sigmoid(logits['logits_utilization'].detach().cpu())> 0.5) & masks['context_mask']\n",
    "    adh_pred  = (torch.sigmoid(logits['logits_adherence'].detach().cpu())   > 0.5) & masks['response_mask']\n",
    "\n",
    "    rel_true  = (labels['labels_relevance']   == 1) & masks['context_mask']\n",
    "    util_true = (labels['labels_utilization']== 1) & masks['context_mask']\n",
    "    adh_true  = (labels['labels_adherence']   == 1) & masks['response_mask']\n",
    "\n",
    "    def prf(pred, true):\n",
    "        tp = (pred & true).sum().float()\n",
    "        fp = (pred & ~true).sum().float()\n",
    "        fn = (~pred & true).sum().float()\n",
    "        prec = tp.div(tp + fp.clamp(min=1))\n",
    "        rec  = tp.div(tp + fn.clamp(min=1))\n",
    "        f1   = 2*prec*rec.div((prec+rec).clamp(min=1))\n",
    "        return prec.item(), rec.item(), f1.item()\n",
    "\n",
    "    rel_p,  rel_r,  rel_f1  = prf(rel_pred,  rel_true)\n",
    "    util_p, util_r, util_f1 = prf(util_pred, util_true)\n",
    "    adh_p,  adh_r,  adh_f1  = prf(adh_pred,  adh_true)\n",
    "\n",
    "    return {\n",
    "        'relevance_precision':    rel_p,\n",
    "        'relevance_recall':       rel_r,\n",
    "        'relevance_f1':           rel_f1,\n",
    "        'utilization_precision':  util_p,\n",
    "        'utilization_recall':     util_r,\n",
    "        'utilization_f1':         util_f1,\n",
    "        'adherence_precision':    adh_p,\n",
    "        'adherence_recall':       adh_r,\n",
    "        'adherence_f1':           adh_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics_per_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8342dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_classification_metrics_stream(all_logits, all_labels, all_masks, threshold=0.5):\n",
    "    \"\"\"\n",
    "    all_logits: dict с ключами 'logits_relevance', 'logits_utilization', 'logits_adherence'\n",
    "                значениями — списки тензоров [B_i, L_i]\n",
    "    all_labels: dict с ключами 'labels_relevance', 'labels_utilization', 'labels_adherence'\n",
    "                значениями — списки тензоров [B_i, L_i]\n",
    "    all_masks:  dict с ключами 'context_mask', 'response_mask'\n",
    "                значениями — списки булевых тензоров [B_i, L_i]\n",
    "    threshold: порог для сигмоиды\n",
    "    \n",
    "    Возвращает dict с precision/recall/F1 для каждой из трёх голов.\n",
    "    \"\"\"\n",
    "    rel_pred_list, rel_true_list = [], []\n",
    "    util_pred_list, util_true_list = [], []\n",
    "    adh_pred_list, adh_true_list   = [], []\n",
    "\n",
    "    for (\n",
    "        lr, lu, la,\n",
    "        tr, tu, ta,\n",
    "        ctx_m, resp_m\n",
    "    ) in zip(\n",
    "        all_logits['logits_relevance'],\n",
    "        all_logits['logits_utilization'],\n",
    "        all_logits['logits_adherence'],\n",
    "        all_labels['labels_relevance'],\n",
    "        all_labels['labels_utilization'],\n",
    "        all_labels['labels_adherence'],\n",
    "        all_masks['context_mask'],\n",
    "        all_masks['response_mask'],\n",
    "    ):\n",
    "        p_rel  = (torch.sigmoid(lr) > threshold)\n",
    "        p_util = (torch.sigmoid(lu) > threshold)\n",
    "        p_adh  = (torch.sigmoid(la) > threshold)\n",
    "        t_rel  = (tr == 1)\n",
    "        t_util = (tu == 1)\n",
    "        t_adh  = (ta == 1)\n",
    "\n",
    "        rel_pred_list.append( p_rel[ctx_m].cpu() )\n",
    "        rel_true_list.append( t_rel[ctx_m].cpu() )\n",
    "        util_pred_list.append( p_util[ctx_m].cpu() )\n",
    "        util_true_list.append( t_util[ctx_m].cpu() )\n",
    "        adh_pred_list.append( p_adh[resp_m].cpu() )\n",
    "        adh_true_list.append( t_adh[resp_m].cpu() )\n",
    "\n",
    "    rel_pred_all  = torch.cat(rel_pred_list)\n",
    "    rel_true_all  = torch.cat(rel_true_list)\n",
    "    util_pred_all = torch.cat(util_pred_list)\n",
    "    util_true_all = torch.cat(util_true_list)\n",
    "    adh_pred_all  = torch.cat(adh_pred_list)\n",
    "    adh_true_all  = torch.cat(adh_true_list)\n",
    "\n",
    "    def prf(pred, true):\n",
    "        tp = int((pred & true).sum())\n",
    "        fp = int((pred & ~true).sum())\n",
    "        fn = int((~pred & true).sum())\n",
    "        prec = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1   = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1\n",
    "\n",
    "    rel_p, rel_r, rel_f1   = prf(rel_pred_all,  rel_true_all)\n",
    "    util_p, util_r, util_f1 = prf(util_pred_all, util_true_all)\n",
    "    adh_p, adh_r, adh_f1    = prf(adh_pred_all,  adh_true_all)\n",
    "\n",
    "    return {\n",
    "        'relevance_precision':    rel_p,\n",
    "        'relevance_recall':       rel_r,\n",
    "        'relevance_f1':           rel_f1,\n",
    "        'utilization_precision':  util_p,\n",
    "        'utilization_recall':     util_r,\n",
    "        'utilization_f1':         util_f1,\n",
    "        'adherence_precision':    adh_p,\n",
    "        'adherence_recall':       adh_r,\n",
    "        'adherence_f1':           adh_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11273c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model=DebertaTrace(base).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "accum=4\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    opt.zero_grad()\n",
    "    loop = tqdm(train_loader, desc=f\"Training Epoch {epoch}\", total=len(train_loader))\n",
    "    for i, batch in enumerate(loop, 1):\n",
    "        inputs = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
    "        labels = {k: v.to(device) for k,v in batch.items() if k.startswith('labels_')}\n",
    "        masks  = {k: v.to(device) for k,v in batch.items() if k.endswith('_mask')}\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            out = model(**inputs)\n",
    "            \n",
    "            l_rel = F.binary_cross_entropy_with_logits(out['logits_relevance'][masks['context_mask']], labels['labels_relevance'][masks['context_mask']])\n",
    "\n",
    "            l_util= F.binary_cross_entropy_with_logits(out['logits_utilization'][masks['context_mask']], labels['labels_utilization'][masks['context_mask']])\n",
    "\n",
    "            l_adh = F.binary_cross_entropy_with_logits(out['logits_adherence'][masks['response_mask']], labels['labels_adherence'][masks['response_mask']])\n",
    "            loss = (l_rel + l_util + l_adh)/ 3 #/ accum\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         if (i+1)%accum==0:\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "#         break\n",
    "    print(f\"Epoch {epoch} loss: {total_loss:.4f}\")\n",
    "\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    sums = np.zeros(4);\n",
    "    count=0\n",
    "    metrics = {\n",
    "    'relevance_rate':   [],  # доля токенов контекста, помеченных релевантными\n",
    "    'utilization_rate': [],  # доля токенов контекста, помеченных используемыми\n",
    "    'adherence_rate':   [],  # доля токенов ответа,   помеченных подкреплёнными\n",
    "    'completeness':     []   # доля релевантных предсказаний, которые ещё и util\n",
    "    }\n",
    "    all_logits = {\n",
    "        'logits_relevance':   [],\n",
    "        'logits_utilization': [],\n",
    "        'logits_adherence':   []\n",
    "    }\n",
    "    all_labels = {\n",
    "        'labels_relevance':   [],\n",
    "        'labels_utilization': [],\n",
    "        'labels_adherence':   []\n",
    "    }\n",
    "    all_masks = {\n",
    "        'context_mask':  [],\n",
    "        'response_mask': []\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in tqdm(val_loader):\n",
    "            inputs = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
    "            labels = {k: v for k,v in batch.items() if k.startswith('labels_')}\n",
    "            masks  = {k: v for k,v in batch.items() if k.endswith('_mask')}\n",
    "            out = model(**{k:inputs[k] for k in inputs})\n",
    "            batch_metrics = compute_trace_metrics_inference(out, masks)\n",
    "#             m = compute_batch_metrics(out, labels, masks)\n",
    "            for name, values in batch_metrics.items():\n",
    "                metrics[name].extend(values.cpu().tolist())\n",
    "#             sums += np.array(m)\n",
    "            for k in all_logits:\n",
    "#                 print(out[k].shape)\n",
    "                all_logits[k].append(out[k].cpu())\n",
    "            for k in all_labels:\n",
    "                all_labels[k].append(labels[k].cpu())\n",
    "            for k in all_masks:\n",
    "                all_masks[k].append(masks[k].cpu())\n",
    "    \n",
    "    big_logits = { k: torch.cat(v, dim=0) for k, v in all_logits.items() }\n",
    "    big_labels = { k: torch.cat(v, dim=0) for k, v in all_labels.items() }\n",
    "    big_masks  = { k: torch.cat(v, dim=0) for k, v in all_masks.items() }\n",
    "    class_metrics = compute_classification_metrics_stream(big_logits, big_labels, big_masks)\n",
    "    print(\"\\nClassification metrics on full validation set:\")\n",
    "    for name, val in class_metrics.items():\n",
    "        print(f\"  {name}: {val:.4f}\")\n",
    "#     avg = sums/count\n",
    "    print(f\"Eval -> relevance={np.mean(metrics['relevance_rate']):.4f}, utilization={np.mean(metrics['utilization_rate']):.4f}, adherence={np.mean(metrics['adherence_rate']):.4f}, completeness={np.mean(metrics['completeness']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12b3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
