{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d4abf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def has_sentences(example):\n",
    "    \"\"\"\n",
    "    Функция возвращает True, если пример содержит хотя бы одно предложение\n",
    "    в documents_sentences (и, опционально, в response_sentences).\n",
    "    Иначе возвращает False.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"documents_sentences\" not in example:\n",
    "        return False\n",
    "    if not isinstance(example[\"documents_sentences\"], list):\n",
    "        return False\n",
    "    total_doc_sentences = 0\n",
    "    for doc in example[\"documents_sentences\"]:\n",
    "        if isinstance(doc, list):\n",
    "            total_doc_sentences += len(doc)\n",
    "    if total_doc_sentences == 0:\n",
    "        return False\n",
    "\n",
    "    if \"response_sentences\" in example:\n",
    "        if not isinstance(example[\"response_sentences\"], list):\n",
    "            return False\n",
    "        if len(example[\"response_sentences\"]) == 0:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "MODEL_NAME = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def preprocess(example, max_length=1024):\n",
    "    question_ids = tokenizer.encode(example[\"question\"], add_special_tokens=False)\n",
    "    doc_ids, rel_labels, util_labels = [], [], []\n",
    "\n",
    "    for doc in example[\"documents_sentences\"]:\n",
    "        for key, sent in doc:\n",
    "            tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
    "            doc_ids += tokens\n",
    "            rel_labels += [float(key in example[\"all_relevant_sentence_keys\"])] * len(tokens)\n",
    "            util_labels += [float(key in example[\"all_utilized_sentence_keys\"])] * len(tokens)\n",
    "\n",
    "    response_ids = tokenizer.encode(example[\"response\"], add_special_tokens=False)\n",
    "    adh_labels = [float(example[\"adherence_score\"])] * len(response_ids)\n",
    "\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    input_ids = question_ids + [sep_id] + doc_ids + [sep_id] + response_ids\n",
    "\n",
    "    context_mask = [0] * (len(question_ids)+1) + [1]*len(doc_ids) + [0] + [0]*len(response_ids)\n",
    "    response_mask = [0]*(len(question_ids)+len(doc_ids)+2) + [1]*len(response_ids)\n",
    "\n",
    "    rel_labels = [0.0]*(len(question_ids)+1) + rel_labels + [0.0]*(len(response_ids)+1)\n",
    "    util_labels = [0.0]*(len(question_ids)+1) + util_labels + [0.0]*(len(response_ids)+1)\n",
    "    adh_labels = [0.0]*(len(question_ids)+len(doc_ids)+2) + adh_labels\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        context_mask = context_mask[:max_length]\n",
    "        response_mask = response_mask[:max_length]\n",
    "        rel_labels = rel_labels[:max_length]\n",
    "        util_labels = util_labels[:max_length]\n",
    "        adh_labels = adh_labels[:max_length]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([1]*len(input_ids), dtype=torch.long),\n",
    "        'context_mask': torch.tensor(context_mask, dtype=torch.bool),\n",
    "        'response_mask': torch.tensor(response_mask, dtype=torch.bool),\n",
    "        'labels_relevance': torch.tensor(rel_labels, dtype=torch.float),\n",
    "        'labels_utilization': torch.tensor(util_labels, dtype=torch.float),\n",
    "        'labels_adherence': torch.tensor(adh_labels, dtype=torch.float),\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b55a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6434dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promt_test.data import load_full_ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09248d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_full_ragbench()#load_dataset(\"rungalileo/ragbench\", \"delucionqa\")\n",
    "dataset = {key:dataset[key].filter(has_sentences) for key in dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d42029",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = tokenizer.model_max_length  # максимально допустимая длина входа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bc5d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|                                                                                                                                                                          | 0/1621 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1621/1621 [00:18<00:00, 88.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(example):\n",
    "    question_ids = tokenizer.encode(example[\"question\"], add_special_tokens=False)\n",
    "    doc_ids = []\n",
    "    for doc in example[\"documents_sentences\"]:\n",
    "        for key, sent in doc:\n",
    "            doc_ids.extend(tokenizer.encode(sent, add_special_tokens=False))\n",
    "    response_ids = tokenizer.encode(example[\"response\"], add_special_tokens=False)\n",
    "    total = len(question_ids) + 1 + len(doc_ids) + 1 + len(response_ids)\n",
    "    return total\n",
    "\n",
    "def filter_by_length(example):\n",
    "    return count_tokens(example) <= MAX_LEN\n",
    "\n",
    "\n",
    "filtered = {\n",
    "    ds_name: ds_dict.filter(filter_by_length)\n",
    "    for ds_name, ds_dict in dataset.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f55d592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_splits = [ds[\"train\"] for ds in filtered.values() if \"train\" in ds]\n",
    "val_splits   = [ds[\"validation\"] for ds in filtered.values() if \"validation\" in ds]\n",
    "test_splits  = [ds[\"test\"] for ds in filtered.values() if \"test\" in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "251a1861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "        num_rows: 26966\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "        num_rows: 3631\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "        num_rows: 4222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "combined = DatasetDict({\n",
    "    \"train\":      concatenate_datasets(train_splits),\n",
    "    \"validation\": concatenate_datasets(val_splits),\n",
    "    \"test\":       concatenate_datasets(test_splits)\n",
    "})\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a6be5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     # batch: list of dicts\n",
    "#     keys = batch[0].keys()\n",
    "#     out = {}\n",
    "#     #  max len\n",
    "#     max_len = MAX_LEN #max(len(x['input_ids']) for x in batch)\n",
    "# #     print(batch)\n",
    "#     for k in keys:\n",
    "#         vals = [x[k] for x in batch]\n",
    "#         if k == 'input_ids':\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "#         elif k == 'attention_mask':\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=0)\n",
    "#         elif k in ['context_mask', 'response_mask']:\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=False)\n",
    "#         elif k.startswith('labels_'):\n",
    "#             out[k] = pad_sequence(vals, batch_first=True, padding_value=0)\n",
    "#         else:\n",
    "#             out[k] = torch.stack(vals)\n",
    "#     return out\n",
    "# train_data = [preprocess(ex) for ex in combined['train']]\n",
    "# val_data   = [preprocess(ex) for ex in combined['validation']]\n",
    "# batch_size=8\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62758648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(x: torch.Tensor, max_len: int, pad_value: int):\n",
    "    L = x.size(0)\n",
    "    if L < max_len:\n",
    "        # дополняем в конец\n",
    "        pad = x.new_full((max_len - L,), pad_value)\n",
    "        return torch.cat([x, pad], dim=0)\n",
    "    else:\n",
    "        # усекаем\n",
    "        return x[:max_len]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    max_len = MAX_LEN\n",
    "\n",
    "    for k in keys:\n",
    "        vals = [x[k] for x in batch]\n",
    "        if k == 'input_ids':\n",
    "            # сначала усекаем/падим каждый, потом стэкаем\n",
    "            processed = [pad_or_truncate(x, max_len, tokenizer.pad_token_id) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        elif k == 'attention_mask':\n",
    "            processed = [pad_or_truncate(x, max_len, 0) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        elif k in ['context_mask', 'response_mask']:\n",
    "            processed = [pad_or_truncate(x.to(torch.uint8), max_len, 0).to(torch.bool) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        elif k.startswith('labels_'):\n",
    "            processed = [pad_or_truncate(x, max_len, 0) for x in vals]\n",
    "            out[k] = torch.stack(processed, dim=0)\n",
    "        else:\n",
    "            out[k] = torch.stack(vals, dim=0)\n",
    "    return out\n",
    "train_data = [preprocess(ex) for ex in combined['train']]\n",
    "val_data   = [preprocess(ex) for ex in combined['validation']]\n",
    "batch_size=8\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75f9f79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['train'][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82082e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35c67bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34462288803319874, 0.2855946234091305, 0.8451311705064115)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(combined['validation']['relevance_score']), np.mean(combined['validation']['utilization_score']), np.mean(combined['validation']['completeness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd353c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                | Relevance | Utilization | Adherence\n",
      "------------------------------------------------------------\n",
      "▁What                |       0.0 |         0.0 |       0.0\n",
      "▁CO                  |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "s                    |       0.0 |         0.0 |       0.0\n",
      "▁were                |       0.0 |         0.0 |       0.0\n",
      "▁known               |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁infect              |       0.0 |         0.0 |       0.0\n",
      "▁humans              |       0.0 |         0.0 |       0.0\n",
      "▁before              |       0.0 |         0.0 |       0.0\n",
      "▁December            |       0.0 |         0.0 |       0.0\n",
      "▁2019                |       0.0 |         0.0 |       0.0\n",
      "?                    |       0.0 |         0.0 |       0.0\n",
      "[SEP]                |       0.0 |         0.0 |       0.0\n",
      "▁Title               |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁Genomic             |       0.0 |         0.0 |       0.0\n",
      "▁characterization    |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁2019                |       0.0 |         0.0 |       0.0\n",
      "▁novel               |       0.0 |         0.0 |       0.0\n",
      "▁human               |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "pathogenic           |       0.0 |         0.0 |       0.0\n",
      "▁corona              |       0.0 |         0.0 |       0.0\n",
      "virus                |       0.0 |         0.0 |       0.0\n",
      "▁isolated            |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁a                   |       0.0 |         0.0 |       0.0\n",
      "▁patient             |       0.0 |         0.0 |       0.0\n",
      "▁with                |       0.0 |         0.0 |       0.0\n",
      "▁atypical            |       0.0 |         0.0 |       0.0\n",
      "▁pneumonia           |       0.0 |         0.0 |       0.0\n",
      "▁after               |       0.0 |         0.0 |       0.0\n",
      "▁visiting            |       0.0 |         0.0 |       0.0\n",
      "▁Wuhan               |       0.0 |         0.0 |       0.0\n",
      "▁Passage             |       1.0 |         1.0 |       0.0\n",
      ":                    |       1.0 |         1.0 |       0.0\n",
      "▁Prior               |       1.0 |         1.0 |       0.0\n",
      "▁to                  |       1.0 |         1.0 |       0.0\n",
      "▁December            |       1.0 |         1.0 |       0.0\n",
      "▁2019                |       1.0 |         1.0 |       0.0\n",
      ",                    |       1.0 |         1.0 |       0.0\n",
      "▁6                   |       1.0 |         1.0 |       0.0\n",
      "▁Co                  |       1.0 |         1.0 |       0.0\n",
      "V                    |       1.0 |         1.0 |       0.0\n",
      "s                    |       1.0 |         1.0 |       0.0\n",
      "▁were                |       1.0 |         1.0 |       0.0\n",
      "▁known               |       1.0 |         1.0 |       0.0\n",
      "▁to                  |       1.0 |         1.0 |       0.0\n",
      "▁infect              |       1.0 |         1.0 |       0.0\n",
      "▁human               |       1.0 |         1.0 |       0.0\n",
      ",                    |       1.0 |         1.0 |       0.0\n",
      "▁including           |       1.0 |         1.0 |       0.0\n",
      "▁2                   |       1.0 |         1.0 |       0.0\n",
      "▁α                   |       1.0 |         1.0 |       0.0\n",
      "Co                   |       1.0 |         1.0 |       0.0\n",
      "V                    |       1.0 |         1.0 |       0.0\n",
      "▁and                 |       1.0 |         1.0 |       0.0\n",
      "▁4                   |       1.0 |         1.0 |       0.0\n",
      "▁β                   |       1.0 |         1.0 |       0.0\n",
      "Co                   |       1.0 |         1.0 |       0.0\n",
      "V                    |       1.0 |         1.0 |       0.0\n",
      "▁(                   |       1.0 |         1.0 |       0.0\n",
      "HC                   |       1.0 |         1.0 |       0.0\n",
      "o                    |       1.0 |         1.0 |       0.0\n",
      "V                    |       1.0 |         1.0 |       0.0\n",
      "-                    |       1.0 |         1.0 |       0.0\n",
      "OC                   |       1.0 |         1.0 |       0.0\n",
      "43                   |       1.0 |         1.0 |       0.0\n",
      "▁[                   |       1.0 |         1.0 |       0.0\n",
      "▁Title               |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁Genomic             |       0.0 |         0.0 |       0.0\n",
      "▁characterization    |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁2019                |       0.0 |         0.0 |       0.0\n",
      "▁novel               |       0.0 |         0.0 |       0.0\n",
      "▁human               |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "pathogenic           |       0.0 |         0.0 |       0.0\n",
      "▁corona              |       0.0 |         0.0 |       0.0\n",
      "virus                |       0.0 |         0.0 |       0.0\n",
      "▁isolated            |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁a                   |       0.0 |         0.0 |       0.0\n",
      "▁patient             |       0.0 |         0.0 |       0.0\n",
      "▁with                |       0.0 |         0.0 |       0.0\n",
      "▁atypical            |       0.0 |         0.0 |       0.0\n",
      "▁pneumonia           |       0.0 |         0.0 |       0.0\n",
      "▁after               |       0.0 |         0.0 |       0.0\n",
      "▁visiting            |       0.0 |         0.0 |       0.0\n",
      "▁Wuhan               |       0.0 |         0.0 |       0.0\n",
      "▁Passage             |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁infections          |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁800                 |       0.0 |         0.0 |       0.0\n",
      "▁deaths              |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁Middle              |       0.0 |         0.0 |       0.0\n",
      "▁East                |       0.0 |         0.0 |       0.0\n",
      "▁respiratory         |       0.0 |         0.0 |       0.0\n",
      "▁syndrome            |       0.0 |         0.0 |       0.0\n",
      "▁Co                  |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "▁which               |       0.0 |         0.0 |       0.0\n",
      "▁has                 |       0.0 |         0.0 |       0.0\n",
      "▁caused              |       0.0 |         0.0 |       0.0\n",
      "▁a                   |       0.0 |         0.0 |       0.0\n",
      "▁persistent          |       0.0 |         0.0 |       0.0\n",
      "▁epidemic            |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁Arabian             |       0.0 |         0.0 |       0.0\n",
      "▁Peninsula           |       0.0 |         0.0 |       0.0\n",
      "▁since               |       0.0 |         0.0 |       0.0\n",
      "▁2012                |       0.0 |         0.0 |       0.0\n",
      "▁.                   |       0.0 |         0.0 |       0.0\n",
      "▁In                  |       0.0 |         0.0 |       0.0\n",
      "▁both                |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁these               |       0.0 |         0.0 |       0.0\n",
      "▁epidemics           |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁these               |       0.0 |         0.0 |       0.0\n",
      "▁viruses             |       0.0 |         0.0 |       0.0\n",
      "▁have                |       0.0 |         0.0 |       0.0\n",
      "▁likely              |       0.0 |         0.0 |       0.0\n",
      "▁originated          |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁bats                |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁then                |       0.0 |         0.0 |       0.0\n",
      "▁jumped              |       0.0 |         0.0 |       0.0\n",
      "▁into                |       0.0 |         0.0 |       0.0\n",
      "▁another             |       0.0 |         0.0 |       0.0\n",
      "▁amplification       |       0.0 |         0.0 |       0.0\n",
      "▁mammalian           |       0.0 |         0.0 |       0.0\n",
      "▁host                |       0.0 |         0.0 |       0.0\n",
      "▁for                 |       0.0 |         0.0 |       0.0\n",
      "▁SARS                |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "Co                   |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁dro                 |       0.0 |         0.0 |       0.0\n",
      "med                  |       0.0 |         0.0 |       0.0\n",
      "ary                  |       0.0 |         0.0 |       0.0\n",
      "▁camel               |       0.0 |         0.0 |       0.0\n",
      "▁for                 |       0.0 |         0.0 |       0.0\n",
      "▁MERS                |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "Co                   |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "]                    |       0.0 |         0.0 |       0.0\n",
      "▁before              |       0.0 |         0.0 |       0.0\n",
      "▁crossing            |       0.0 |         0.0 |       0.0\n",
      "▁species             |       0.0 |         0.0 |       0.0\n",
      "▁barriers            |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁infect              |       0.0 |         0.0 |       0.0\n",
      "▁humans              |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁Title               |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁Potential           |       0.0 |         0.0 |       0.0\n",
      "▁Maternal            |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁Infant              |       0.0 |         0.0 |       0.0\n",
      "▁Outcomes            |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁(                   |       0.0 |         0.0 |       0.0\n",
      "W                    |       0.0 |         0.0 |       0.0\n",
      "uhan                 |       0.0 |         0.0 |       0.0\n",
      ")                    |       0.0 |         0.0 |       0.0\n",
      "▁Corona              |       0.0 |         0.0 |       0.0\n",
      "virus                |       0.0 |         0.0 |       0.0\n",
      "▁2019                |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "n                    |       0.0 |         0.0 |       0.0\n",
      "Co                   |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "▁Infect              |       0.0 |         0.0 |       0.0\n",
      "ing                  |       0.0 |         0.0 |       0.0\n",
      "▁Pregnant            |       0.0 |         0.0 |       0.0\n",
      "▁Women               |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁Lessons             |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁SARS                |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁MERS                |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁Other               |       0.0 |         0.0 |       0.0\n",
      "▁Human               |       0.0 |         0.0 |       0.0\n",
      "▁Corona              |       0.0 |         0.0 |       0.0\n",
      "virus                |       0.0 |         0.0 |       0.0\n",
      "▁Infections          |       0.0 |         0.0 |       0.0\n",
      "▁Passage             |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁The                 |       0.0 |         0.0 |       0.0\n",
      "▁SARS                |       0.0 |         0.0 |       0.0\n",
      "▁epidemic            |       0.0 |         0.0 |       0.0\n",
      "▁began               |       0.0 |         0.0 |       0.0\n",
      "▁quietly             |       0.0 |         0.0 |       0.0\n",
      "▁at                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁turn                |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁21                  |       0.0 |         0.0 |       0.0\n",
      "st                   |       0.0 |         0.0 |       0.0\n",
      "▁century             |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁In                  |       0.0 |         0.0 |       0.0\n",
      "▁November            |       0.0 |         0.0 |       0.0\n",
      "▁2002                |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁a                   |       0.0 |         0.0 |       0.0\n",
      "▁cook                |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁Guangdong           |       0.0 |         0.0 |       0.0\n",
      "▁Province            |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁China               |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁died                |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁an                  |       0.0 |         0.0 |       0.0\n",
      "▁unidentified        |       0.0 |         0.0 |       0.0\n",
      "▁illness             |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁He                  |       0.0 |         0.0 |       0.0\n",
      "▁had                 |       0.0 |         0.0 |       0.0\n",
      "▁worked              |       0.0 |         0.0 |       0.0\n",
      "▁at                  |       0.0 |         0.0 |       0.0\n",
      "▁a                   |       0.0 |         0.0 |       0.0\n",
      "▁restaurant          |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁which               |       0.0 |         0.0 |       0.0\n",
      "▁meat                |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁wild                |       0.0 |         0.0 |       0.0\n",
      "▁animals             |       0.0 |         0.0 |       0.0\n",
      "▁was                 |       0.0 |         0.0 |       0.0\n",
      "▁served              |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁On                  |       0.0 |         0.0 |       0.0\n",
      "▁27                  |       0.0 |         0.0 |       0.0\n",
      "▁November            |       0.0 |         0.0 |       0.0\n",
      "▁2002                |       0.0 |         0.0 |       0.0\n",
      "▁Chinese             |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "language             |       0.0 |         0.0 |       0.0\n",
      "▁media               |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁internet            |       0.0 |         0.0 |       0.0\n",
      "▁reports             |       0.0 |         0.0 |       0.0\n",
      "▁were                |       0.0 |         0.0 |       0.0\n",
      "▁picked              |       0.0 |         0.0 |       0.0\n",
      "▁up                  |       0.0 |         0.0 |       0.0\n",
      "▁by                  |       0.0 |         0.0 |       0.0\n",
      "▁Canada              |       0.0 |         0.0 |       0.0\n",
      "'                    |       0.0 |         0.0 |       0.0\n",
      "s                    |       0.0 |         0.0 |       0.0\n",
      "▁Global              |       0.0 |         0.0 |       0.0\n",
      "▁Public              |       0.0 |         0.0 |       0.0\n",
      "▁Health              |       0.0 |         0.0 |       0.0\n",
      "▁Intelligence        |       0.0 |         0.0 |       0.0\n",
      "▁Network             |       0.0 |         0.0 |       0.0\n",
      "▁that                |       0.0 |         0.0 |       0.0\n",
      "▁indicated           |       0.0 |         0.0 |       0.0\n",
      "▁a                   |       0.0 |         0.0 |       0.0\n",
      "▁flu                 |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "like                 |       0.0 |         0.0 |       0.0\n",
      "▁illness             |       0.0 |         0.0 |       0.0\n",
      "▁was                 |       0.0 |         0.0 |       0.0\n",
      "▁occurring           |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁China               |       0.0 |         0.0 |       0.0\n",
      "▁.                   |       0.0 |         0.0 |       0.0\n",
      "▁Unfortunately       |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁reports             |       0.0 |         0.0 |       0.0\n",
      "▁were                |       0.0 |         0.0 |       0.0\n",
      "▁not                 |       0.0 |         0.0 |       0.0\n",
      "▁translated          |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁China               |       0.0 |         0.0 |       0.0\n",
      "▁failed              |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁report              |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁occurrence          |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁this                |       0.0 |         0.0 |       0.0\n",
      "▁illness             |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁World               |       0.0 |         0.0 |       0.0\n",
      "▁Health              |       0.0 |         0.0 |       0.0\n",
      "▁Organization        |       0.0 |         0.0 |       0.0\n",
      "▁until               |       0.0 |         0.0 |       0.0\n",
      "▁February            |       0.0 |         0.0 |       0.0\n",
      "▁2003                |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁The                 |       0.0 |         0.0 |       0.0\n",
      "▁disease             |       0.0 |         0.0 |       0.0\n",
      "▁spread              |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁other               |       0.0 |         0.0 |       0.0\n",
      "▁countries           |       0.0 |         0.0 |       0.0\n",
      "▁where               |       0.0 |         0.0 |       0.0\n",
      "▁Title               |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁Molecular           |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁serological         |       0.0 |         0.0 |       0.0\n",
      "▁investigation       |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁2019                |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "n                    |       0.0 |         0.0 |       0.0\n",
      "Co                   |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "▁infected            |       0.0 |         0.0 |       0.0\n",
      "▁patients            |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁implication         |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁multiple            |       0.0 |         0.0 |       0.0\n",
      "▁shedding            |       0.0 |         0.0 |       0.0\n",
      "▁routes              |       0.0 |         0.0 |       0.0\n",
      "▁Passage             |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁Text                |       0.0 |         0.0 |       0.0\n",
      ":                    |       0.0 |         0.0 |       0.0\n",
      "▁Corona              |       0.0 |         0.0 |       0.0\n",
      "viruses              |       0.0 |         0.0 |       0.0\n",
      "▁belong              |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁subfamily           |       0.0 |         0.0 |       0.0\n",
      "▁Ortho               |       0.0 |         0.0 |       0.0\n",
      "cor                  |       0.0 |         0.0 |       0.0\n",
      "on                   |       0.0 |         0.0 |       0.0\n",
      "avir                 |       0.0 |         0.0 |       0.0\n",
      "inae                 |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁family              |       0.0 |         0.0 |       0.0\n",
      "▁Corona              |       0.0 |         0.0 |       0.0\n",
      "vir                  |       0.0 |         0.0 |       0.0\n",
      "idae                 |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁order               |       0.0 |         0.0 |       0.0\n",
      "▁Nido                |       0.0 |         0.0 |       0.0\n",
      "viral                |       0.0 |         0.0 |       0.0\n",
      "es                   |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁A                   |       0.0 |         0.0 |       0.0\n",
      "▁human               |       0.0 |         0.0 |       0.0\n",
      "▁corona              |       0.0 |         0.0 |       0.0\n",
      "virus                |       0.0 |         0.0 |       0.0\n",
      "▁caused              |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁severe              |       0.0 |         0.0 |       0.0\n",
      "▁acute               |       0.0 |         0.0 |       0.0\n",
      "▁respiratory         |       0.0 |         0.0 |       0.0\n",
      "▁syndrome            |       0.0 |         0.0 |       0.0\n",
      "▁corona              |       0.0 |         0.0 |       0.0\n",
      "virus                |       0.0 |         0.0 |       0.0\n",
      "▁outbreak            |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁2003                |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁Most                |       0.0 |         0.0 |       0.0\n",
      "▁recently            |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁an                  |       0.0 |         0.0 |       0.0\n",
      "▁SARS                |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "related              |       0.0 |         0.0 |       0.0\n",
      "▁Co                  |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "▁was                 |       0.0 |         0.0 |       0.0\n",
      "▁implicated          |       0.0 |         0.0 |       0.0\n",
      "▁as                  |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁etiologic           |       0.0 |         0.0 |       0.0\n",
      "al                   |       0.0 |         0.0 |       0.0\n",
      "▁agent               |       0.0 |         0.0 |       0.0\n",
      "▁responsible         |       0.0 |         0.0 |       0.0\n",
      "▁for                 |       0.0 |         0.0 |       0.0\n",
      "▁the                 |       0.0 |         0.0 |       0.0\n",
      "▁outbreak            |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁Wuhan               |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁central             |       0.0 |         0.0 |       0.0\n",
      "▁China               |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁This                |       0.0 |         0.0 |       0.0\n",
      "▁outbreak            |       0.0 |         0.0 |       0.0\n",
      "▁is                  |       0.0 |         0.0 |       0.0\n",
      "▁estimated           |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁have                |       0.0 |         0.0 |       0.0\n",
      "▁started             |       0.0 |         0.0 |       0.0\n",
      "▁on                  |       0.0 |         0.0 |       0.0\n",
      "▁12                  |       0.0 |         0.0 |       0.0\n",
      "th                   |       0.0 |         0.0 |       0.0\n",
      "▁December            |       0.0 |         0.0 |       0.0\n",
      "▁2019                |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁17                  |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "332                  |       0.0 |         0.0 |       0.0\n",
      "▁laboratory          |       0.0 |         0.0 |       0.0\n",
      "▁confirmed           |       0.0 |         0.0 |       0.0\n",
      "▁cases               |       0.0 |         0.0 |       0.0\n",
      "▁with                |       0.0 |         0.0 |       0.0\n",
      "▁361                 |       0.0 |         0.0 |       0.0\n",
      "▁deaths              |       0.0 |         0.0 |       0.0\n",
      "▁as                  |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁3                   |       0.0 |         0.0 |       0.0\n",
      "rd                   |       0.0 |         0.0 |       0.0\n",
      "▁February            |       0.0 |         0.0 |       0.0\n",
      "▁2020                |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁China               |       0.0 |         0.0 |       0.0\n",
      "▁.                   |       0.0 |         0.0 |       0.0\n",
      "▁The                 |       0.0 |         0.0 |       0.0\n",
      "▁virus               |       0.0 |         0.0 |       0.0\n",
      "▁has                 |       0.0 |         0.0 |       0.0\n",
      "▁spread              |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁23                  |       0.0 |         0.0 |       0.0\n",
      "▁other               |       0.0 |         0.0 |       0.0\n",
      "▁countries           |       0.0 |         0.0 |       0.0\n",
      "▁by                  |       0.0 |         0.0 |       0.0\n",
      "▁travellers          |       0.0 |         0.0 |       0.0\n",
      "▁from                |       0.0 |         0.0 |       0.0\n",
      "▁Wuhan               |       0.0 |         0.0 |       0.0\n",
      "▁.                   |       0.0 |         0.0 |       0.0\n",
      "▁Typical             |       0.0 |         0.0 |       0.0\n",
      "▁symptoms            |       0.0 |         0.0 |       0.0\n",
      "▁are                 |       0.0 |         0.0 |       0.0\n",
      "▁fever               |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁malaise             |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁shortness           |       0.0 |         0.0 |       0.0\n",
      "▁of                  |       0.0 |         0.0 |       0.0\n",
      "▁breath              |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁in                  |       0.0 |         0.0 |       0.0\n",
      "▁severe              |       0.0 |         0.0 |       0.0\n",
      "▁cases               |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁pneumonia           |       0.0 |         0.0 |       0.0\n",
      "[SEP]                |       0.0 |         0.0 |       0.0\n",
      "▁Before              |       0.0 |         0.0 |       0.0\n",
      "▁December            |       0.0 |         0.0 |       0.0\n",
      "▁2019                |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁6                   |       0.0 |         0.0 |       0.0\n",
      "▁Co                  |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "s                    |       0.0 |         0.0 |       0.0\n",
      "▁were                |       0.0 |         0.0 |       0.0\n",
      "▁known               |       0.0 |         0.0 |       0.0\n",
      "▁to                  |       0.0 |         0.0 |       0.0\n",
      "▁infect              |       0.0 |         0.0 |       0.0\n",
      "▁humans              |       0.0 |         0.0 |       0.0\n",
      ",                    |       0.0 |         0.0 |       0.0\n",
      "▁including           |       0.0 |         0.0 |       0.0\n",
      "▁2                   |       0.0 |         0.0 |       0.0\n",
      "▁α                   |       0.0 |         0.0 |       0.0\n",
      "Co                   |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁4                   |       0.0 |         0.0 |       0.0\n",
      "▁β                   |       0.0 |         0.0 |       0.0\n",
      "Co                   |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n",
      "▁These               |       0.0 |         0.0 |       0.0\n",
      "▁include             |       0.0 |         0.0 |       0.0\n",
      "▁H                   |       0.0 |         0.0 |       0.0\n",
      "Co                   |       0.0 |         0.0 |       0.0\n",
      "V                    |       0.0 |         0.0 |       0.0\n",
      "-                    |       0.0 |         0.0 |       0.0\n",
      "OC                   |       0.0 |         0.0 |       0.0\n",
      "43                   |       0.0 |         0.0 |       0.0\n",
      "▁and                 |       0.0 |         0.0 |       0.0\n",
      "▁others              |       0.0 |         0.0 |       0.0\n",
      ".                    |       0.0 |         0.0 |       0.0\n"
     ]
    }
   ],
   "source": [
    "processed = preprocess(combined['train'][1])\n",
    "\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(processed[\"input_ids\"])\n",
    "\n",
    "labels_rel = processed[\"labels_relevance\"]\n",
    "labels_util = processed[\"labels_utilization\"]\n",
    "labels_adh = processed[\"labels_adherence\"]\n",
    "\n",
    "print(f\"{'Token':20} | {'Relevance':9} | {'Utilization':11} | {'Adherence':9}\")\n",
    "print(\"-\" * 60)\n",
    "i = 0\n",
    "for token, rel, util, adh in zip(tokens, labels_rel, labels_util, labels_adh):\n",
    "    if token == '[PAD]':\n",
    "        break\n",
    "    i+=1\n",
    "    print(f\"{token:20} | {rel:9.1f} | {util:11.1f} | {adh:9.1f}\")\n",
    "    if token == '▁Simply':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1441c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "class DebertaTrace(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hid = base_model.config.hidden_size\n",
    "        self.rel_head = nn.Linear(hid,1)\n",
    "        self.util_head = nn.Linear(hid,1)\n",
    "        self.adh_head = nn.Linear(hid,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hs = out.last_hidden_state\n",
    "        return {\n",
    "            'logits_relevance': self.rel_head(hs).squeeze(-1),\n",
    "            'logits_utilization': self.util_head(hs).squeeze(-1),\n",
    "            'logits_adherence': self.adh_head(hs).squeeze(-1)\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_batch_metrics(logits, labels, masks):\n",
    "    # logits: dict of tensors [B,L]\n",
    "    # labels: dict of tensors [B,L]\n",
    "    # masks: dict of bool tensors [B,L]\n",
    "    rel_log, util_log, adh_log = logits['logits_relevance'], logits['logits_utilization'], logits['logits_adherence']\n",
    "    rel_lab, util_lab, adh_lab = labels['labels_relevance'], labels['labels_utilization'], labels['labels_adherence']\n",
    "    ctx_m, resp_m = masks['context_mask'], masks['response_mask']\n",
    "\n",
    "    # probs\n",
    "    rel_p = torch.tensor(expit(rel_log.detach().cpu().numpy()))\n",
    "    util_p = torch.tensor(expit(util_log.detach().cpu().numpy()))\n",
    "    adh_p = torch.tensor(expit(adh_log.detach().cpu().numpy()))\n",
    "    # preds\n",
    "    pred_rel = rel_p>0.5\n",
    "    pred_util = util_p>0.5\n",
    "    pred_adh = adh_p>0.5\n",
    "    # true\n",
    "    true_rel = rel_lab.detach().cpu()==1\n",
    "    true_util = util_lab.detach().cpu()==1\n",
    "    true_adh = adh_lab.detach().cpu()==1\n",
    "\n",
    "    # per example\n",
    "    def ex_metric(pred, true, mask):\n",
    "        inter = (pred & true & mask).sum(dim=1).float()\n",
    "        denom = (true & mask).sum(dim=1).float()\n",
    "        return (inter/denom.clamp(min=1)).mean().item()\n",
    "\n",
    "    rel_m = ex_metric(pred_rel, true_rel, ctx_m)\n",
    "    util_m= ex_metric(pred_util, true_util, ctx_m)\n",
    "    adh_m = ex_metric(pred_adh, true_adh, resp_m)\n",
    "    comp = ((pred_rel & pred_util & ctx_m).sum(dim=1).float()/ (true_rel & ctx_m).sum(dim=1).float().clamp(min=1)).mean().item()\n",
    "    return rel_m, util_m, adh_m, comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4182118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(logits, labels, mask, head_name=\"\"):\n",
    "    logit_vals = logits[mask]\n",
    "    label_vals = labels[mask]\n",
    "    \n",
    "    nan_logits = torch.isnan(logit_vals)\n",
    "    nan_labels = torch.isnan(label_vals)\n",
    "    \n",
    "    if nan_logits.any() or nan_labels.any():\n",
    "        print(f\"NaN detected in head '{head_name}':\")\n",
    "        if nan_logits.any():\n",
    "            idxs = torch.nonzero(nan_logits, as_tuple=True)[0]\n",
    "            print(f\"  logits NaN at positions: {idxs.tolist()}\")\n",
    "            print(f\"  sample logits: {logit_vals[idxs].tolist()}\")\n",
    "        if nan_labels.any():\n",
    "            idxs = torch.nonzero(nan_labels, as_tuple=True)[0]\n",
    "            print(f\"  labels NaN at positions: {idxs.tolist()}\")\n",
    "            print(f\"  sample labels: {label_vals[idxs].tolist()}\")\n",
    "        # Остановим обучение для отладки\n",
    "        raise ValueError(f\"NaN encountered in head '{head_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2545d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trace_metrics_inference(logits, masks, threshold=0.5):\n",
    "\n",
    "    rel_pred  = (torch.sigmoid(logits['logits_relevance'].detach().cpu())  > threshold)\n",
    "    util_pred = (torch.sigmoid(logits['logits_utilization'].detach().cpu())> threshold)\n",
    "    adh_pred  = (torch.sigmoid(logits['logits_adherence'].detach().cpu())   > threshold)\n",
    "\n",
    "    ctx_m  = masks['context_mask'].detach().cpu()\n",
    "    resp_m = masks['response_mask'].detach().cpu()\n",
    "\n",
    "    def rate(pred, mask):\n",
    "        # sum(pred & mask) / sum(mask)\n",
    "        num = (pred & mask).sum(dim=1).float()\n",
    "        den = mask.sum(dim=1).float().clamp(min=1)\n",
    "        return num.div(den)\n",
    "\n",
    "    relevance_rate   = rate(rel_pred,  ctx_m)\n",
    "    utilization_rate = rate(util_pred, ctx_m)\n",
    "    adherence_rate   = rate(adh_pred,  resp_m)\n",
    "\n",
    "    # completeness: из релевантных предсказаний — сколько ещё и util\n",
    "    num_ru = (rel_pred & util_pred & ctx_m).sum(dim=1).float()\n",
    "    den_r  = rel_pred.sum(dim=1).float().clamp(min=1)\n",
    "    completeness = num_ru.div(den_r)\n",
    "\n",
    "    return {\n",
    "        'relevance_rate':   relevance_rate,    \n",
    "        'utilization_rate': utilization_rate,  \n",
    "        'adherence_rate':   adherence_rate,\n",
    "        'completeness':     completeness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1a8fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(logits, labels, masks):\n",
    "    rel_pred  = (torch.sigmoid(logits['logits_relevance'].detach().cpu())  > 0.5) & masks['context_mask']\n",
    "    util_pred = (torch.sigmoid(logits['logits_utilization'].detach().cpu())> 0.5) & masks['context_mask']\n",
    "    adh_pred  = (torch.sigmoid(logits['logits_adherence'].detach().cpu())   > 0.5) & masks['response_mask']\n",
    "\n",
    "    rel_true  = (labels['labels_relevance']   == 1) & masks['context_mask']\n",
    "    util_true = (labels['labels_utilization']== 1) & masks['context_mask']\n",
    "    adh_true  = (labels['labels_adherence']   == 1) & masks['response_mask']\n",
    "\n",
    "    def prf(pred, true):\n",
    "        tp = (pred & true).sum().float()\n",
    "        fp = (pred & ~true).sum().float()\n",
    "        fn = (~pred & true).sum().float()\n",
    "        prec = tp.div(tp + fp.clamp(min=1))\n",
    "        rec  = tp.div(tp + fn.clamp(min=1))\n",
    "        f1   = 2*prec*rec.div((prec+rec).clamp(min=1))\n",
    "        return prec.item(), rec.item(), f1.item()\n",
    "\n",
    "    rel_p,  rel_r,  rel_f1  = prf(rel_pred,  rel_true)\n",
    "    util_p, util_r, util_f1 = prf(util_pred, util_true)\n",
    "    adh_p,  adh_r,  adh_f1  = prf(adh_pred,  adh_true)\n",
    "\n",
    "    return {\n",
    "        'relevance_precision':    rel_p,\n",
    "        'relevance_recall':       rel_r,\n",
    "        'relevance_f1':           rel_f1,\n",
    "        'utilization_precision':  util_p,\n",
    "        'utilization_recall':     util_r,\n",
    "        'utilization_f1':         util_f1,\n",
    "        'adherence_precision':    adh_p,\n",
    "        'adherence_recall':       adh_r,\n",
    "        'adherence_f1':           adh_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77438b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics_per_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3525a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_classification_metrics_stream(all_logits, all_labels, all_masks, threshold=0.5):\n",
    "    rel_pred_list, rel_true_list = [], []\n",
    "    util_pred_list, util_true_list = [], []\n",
    "    adh_pred_list, adh_true_list   = [], []\n",
    "\n",
    "    for (\n",
    "        lr, lu, la,\n",
    "        tr, tu, ta,\n",
    "        ctx_m, resp_m\n",
    "    ) in zip(\n",
    "        all_logits['logits_relevance'],\n",
    "        all_logits['logits_utilization'],\n",
    "        all_logits['logits_adherence'],\n",
    "        all_labels['labels_relevance'],\n",
    "        all_labels['labels_utilization'],\n",
    "        all_labels['labels_adherence'],\n",
    "        all_masks['context_mask'],\n",
    "        all_masks['response_mask'],\n",
    "    ):\n",
    "        p_rel  = (torch.sigmoid(lr) > threshold)\n",
    "        p_util = (torch.sigmoid(lu) > threshold)\n",
    "        p_adh  = (torch.sigmoid(la) > threshold)\n",
    "        t_rel  = (tr == 1)\n",
    "        t_util = (tu == 1)\n",
    "        t_adh  = (ta == 1)\n",
    "\n",
    "        rel_pred_list.append( p_rel[ctx_m].cpu() )\n",
    "        rel_true_list.append( t_rel[ctx_m].cpu() )\n",
    "        util_pred_list.append( p_util[ctx_m].cpu() )\n",
    "        util_true_list.append( t_util[ctx_m].cpu() )\n",
    "        adh_pred_list.append( p_adh[resp_m].cpu() )\n",
    "        adh_true_list.append( t_adh[resp_m].cpu() )\n",
    "\n",
    "    rel_pred_all  = torch.cat(rel_pred_list)\n",
    "    rel_true_all  = torch.cat(rel_true_list)\n",
    "    util_pred_all = torch.cat(util_pred_list)\n",
    "    util_true_all = torch.cat(util_true_list)\n",
    "    adh_pred_all  = torch.cat(adh_pred_list)\n",
    "    adh_true_all  = torch.cat(adh_true_list)\n",
    "\n",
    "    def prf(pred, true):\n",
    "        tp = int((pred & true).sum())\n",
    "        fp = int((pred & ~true).sum())\n",
    "        fn = int((~pred & true).sum())\n",
    "        prec = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1   = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1\n",
    "\n",
    "    rel_p, rel_r, rel_f1   = prf(rel_pred_all,  rel_true_all)\n",
    "    util_p, util_r, util_f1 = prf(util_pred_all, util_true_all)\n",
    "    adh_p, adh_r, adh_f1    = prf(adh_pred_all,  adh_true_all)\n",
    "\n",
    "    return {\n",
    "        'relevance_precision':    rel_p,\n",
    "        'relevance_recall':       rel_r,\n",
    "        'relevance_f1':           rel_f1,\n",
    "        'utilization_precision':  util_p,\n",
    "        'utilization_recall':     util_r,\n",
    "        'utilization_f1':         util_f1,\n",
    "        'adherence_precision':    adh_p,\n",
    "        'adherence_recall':       adh_r,\n",
    "        'adherence_f1':           adh_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf7debdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model=DebertaTrace(base).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "accum=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff1f8545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c62ff0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/huggingface_cli.py\", line 51, in main\n",
      "    service.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 115, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71714fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model_repo/tokenizer_config.json',\n",
       " '../model_repo/special_tokens_map.json',\n",
       " '../model_repo/spm.model',\n",
       " '../model_repo/added_tokens.json',\n",
       " '../model_repo/tokenizer.json')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = DebertaTrace(base_model)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"../tmp/model_state_dict.pth\", weights_only=True))\n",
    "\n",
    "# base_model.save_pretrained(\"../model_repo\")\n",
    "\n",
    "# torch.save({\n",
    "#     'rel_head': model.rel_head.state_dict(),\n",
    "#     'util_head': model.util_head.state_dict(),\n",
    "#     'adh_head': model.adh_head.state_dict()\n",
    "# }, \"../model_repo/heads_weights.pt\")\n",
    "# tokenizer.save_pretrained(\"../model_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b8ab841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spm.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.46M/2.46M [00:00<00:00, 3.28MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/CMCenjoyer/deberta-trace/commit/d77afe2c560a6573722a2f662237ff319b063c69', commit_message='Upload folder using huggingface_hub', commit_description='', oid='d77afe2c560a6573722a2f662237ff319b063c69', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "\n",
    "# \n",
    "# api = HfApi()\n",
    "\n",
    "# folder_path = \"../model_repo\"\n",
    "\n",
    "# repo_id = \"CMCenjoyer/deberta-trace\"\n",
    "# api.create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
    "# api.upload_folder(\n",
    "#     folder_path=folder_path,\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=\"model\",\n",
    "#     token=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9aafd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "182a6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3371/3371 [15:41<00:00,  3.58it/s, loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 1063.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [00:47<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification metrics on full validation set:\n",
      "  relevance_precision: 0.6637\n",
      "  relevance_recall: 0.9367\n",
      "  relevance_f1: 0.7769\n",
      "  utilization_precision: 0.7290\n",
      "  utilization_recall: 0.9272\n",
      "  utilization_f1: 0.8162\n",
      "  adherence_precision: 0.7859\n",
      "  adherence_recall: 0.9994\n",
      "  adherence_f1: 0.8799\n",
      "Eval -> relevance=0.5682, utilization=0.4437, adherence=0.9970, completeness=0.3126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3371/3371 [15:42<00:00,  3.58it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 862.5781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [00:47<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification metrics on full validation set:\n",
      "  relevance_precision: 0.6590\n",
      "  relevance_recall: 0.9198\n",
      "  relevance_f1: 0.7679\n",
      "  utilization_precision: 0.7327\n",
      "  utilization_recall: 0.9116\n",
      "  utilization_f1: 0.8125\n",
      "  adherence_precision: 0.8515\n",
      "  adherence_recall: 0.9107\n",
      "  adherence_f1: 0.8801\n",
      "Eval -> relevance=0.5574, utilization=0.4331, adherence=0.9106, completeness=0.3325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3371/3371 [15:41<00:00,  3.58it/s, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 846.2181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [00:47<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification metrics on full validation set:\n",
      "  relevance_precision: 0.6673\n",
      "  relevance_recall: 0.9372\n",
      "  relevance_f1: 0.7795\n",
      "  utilization_precision: 0.7380\n",
      "  utilization_recall: 0.9288\n",
      "  utilization_f1: 0.8225\n",
      "  adherence_precision: 0.8165\n",
      "  adherence_recall: 0.9809\n",
      "  adherence_f1: 0.8912\n",
      "Eval -> relevance=0.5634, utilization=0.4372, adherence=0.9578, completeness=0.3102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3371/3371 [15:42<00:00,  3.58it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 620.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [00:47<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification metrics on full validation set:\n",
      "  relevance_precision: 0.7031\n",
      "  relevance_recall: 0.9177\n",
      "  relevance_f1: 0.7962\n",
      "  utilization_precision: 0.7402\n",
      "  utilization_recall: 0.9224\n",
      "  utilization_f1: 0.8213\n",
      "  adherence_precision: 0.8987\n",
      "  adherence_recall: 0.8805\n",
      "  adherence_f1: 0.8895\n",
      "Eval -> relevance=0.5241, utilization=0.4332, adherence=0.8508, completeness=0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3371/3371 [15:41<00:00,  3.58it/s, loss=0.0737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 444.0855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [00:47<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification metrics on full validation set:\n",
      "  relevance_precision: 0.7078\n",
      "  relevance_recall: 0.9131\n",
      "  relevance_f1: 0.7975\n",
      "  utilization_precision: 0.7448\n",
      "  utilization_recall: 0.9291\n",
      "  utilization_f1: 0.8268\n",
      "  adherence_precision: 0.8770\n",
      "  adherence_recall: 0.9249\n",
      "  adherence_f1: 0.9003\n",
      "Eval -> relevance=0.5170, utilization=0.4333, adherence=0.8913, completeness=0.3332\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    opt.zero_grad()\n",
    "    loop = tqdm(train_loader, desc=f\"Training Epoch {epoch}\", total=len(train_loader))\n",
    "    for i, batch in enumerate(loop, 1):\n",
    "        inputs = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
    "        labels = {k: v.to(device) for k,v in batch.items() if k.startswith('labels_')}\n",
    "        masks  = {k: v.to(device) for k,v in batch.items() if k.endswith('_mask')}\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            out = model(**inputs)\n",
    "            \n",
    "            l_rel = F.binary_cross_entropy_with_logits(out['logits_relevance'][masks['context_mask']], labels['labels_relevance'][masks['context_mask']])\n",
    "\n",
    "            l_util= F.binary_cross_entropy_with_logits(out['logits_utilization'][masks['context_mask']], labels['labels_utilization'][masks['context_mask']])\n",
    "\n",
    "            l_adh = F.binary_cross_entropy_with_logits(out['logits_adherence'][masks['response_mask']], labels['labels_adherence'][masks['response_mask']])\n",
    "            loss = (l_rel + l_util + l_adh)/ 3 #/ accum\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         if (i+1)%accum==0:\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "#         break\n",
    "    print(f\"Epoch {epoch} loss: {total_loss:.4f}\")\n",
    "\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    sums = np.zeros(4);\n",
    "    count=0\n",
    "    metrics = {\n",
    "    'relevance_rate':   [],  \n",
    "    'utilization_rate': [],\n",
    "    'adherence_rate':   [],\n",
    "    'completeness':     []\n",
    "    }\n",
    "    all_logits = {\n",
    "        'logits_relevance':   [],\n",
    "        'logits_utilization': [],\n",
    "        'logits_adherence':   []\n",
    "    }\n",
    "    all_labels = {\n",
    "        'labels_relevance':   [],\n",
    "        'labels_utilization': [],\n",
    "        'labels_adherence':   []\n",
    "    }\n",
    "    all_masks = {\n",
    "        'context_mask':  [],\n",
    "        'response_mask': []\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in tqdm(val_loader):\n",
    "            inputs = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
    "            labels = {k: v for k,v in batch.items() if k.startswith('labels_')}\n",
    "            masks  = {k: v for k,v in batch.items() if k.endswith('_mask')}\n",
    "            out = model(**{k:inputs[k] for k in inputs})\n",
    "            batch_metrics = compute_trace_metrics_inference(out, masks)\n",
    "#             m = compute_batch_metrics(out, labels, masks)\n",
    "            for name, values in batch_metrics.items():\n",
    "                metrics[name].extend(values.cpu().tolist())\n",
    "#             sums += np.array(m)\n",
    "            for k in all_logits:\n",
    "#                 print(out[k].shape)\n",
    "                all_logits[k].append(out[k].cpu())\n",
    "            for k in all_labels:\n",
    "                all_labels[k].append(labels[k].cpu())\n",
    "            for k in all_masks:\n",
    "                all_masks[k].append(masks[k].cpu())\n",
    "    \n",
    "    big_logits = { k: torch.cat(v, dim=0) for k, v in all_logits.items() }\n",
    "    big_labels = { k: torch.cat(v, dim=0) for k, v in all_labels.items() }\n",
    "    big_masks  = { k: torch.cat(v, dim=0) for k, v in all_masks.items() }\n",
    "    class_metrics = compute_classification_metrics(big_logits, big_labels, big_masks)\n",
    "    print(\"\\nClassification metrics on full validation set:\")\n",
    "    for name, val in class_metrics.items():\n",
    "        print(f\"  {name}: {val:.4f}\")\n",
    "#     avg = sums/count\n",
    "    print(f\"Eval -> relevance={np.mean(metrics['relevance_rate']):.4f}, utilization={np.mean(metrics['utilization_rate']):.4f}, adherence={np.mean(metrics['adherence_rate']):.4f}, completeness={np.mean(metrics['completeness']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84280d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification metrics on full validation set:\n",
    "  relevance_precision: 0.7078\n",
    "  relevance_recall: 0.9131\n",
    "  relevance_f1: 0.7975\n",
    "  utilization_precision: 0.7448\n",
    "  utilization_recall: 0.9291\n",
    "  utilization_f1: 0.8268\n",
    "  adherence_precision: 0.8770\n",
    "  adherence_recall: 0.9249\n",
    "  adherence_f1: 0.9003\n",
    "Eval -> relevance=0.5170, utilization=0.4333, adherence=0.8913, completeness=0.3332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cd351fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.save(model.state_dict(), '../tmp/model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6da71fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8672/777383360.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../tmp/model_state_dict.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../tmp/model_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc78e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [00:47<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification metrics on full validation set:\n",
      "  relevance_precision: 0.6925\n",
      "  relevance_recall: 0.9221\n",
      "  relevance_f1: 0.7910\n",
      "  utilization_precision: 0.7372\n",
      "  utilization_recall: 0.9340\n",
      "  utilization_f1: 0.8240\n",
      "  adherence_precision: 0.8590\n",
      "  adherence_recall: 0.9343\n",
      "  adherence_f1: 0.8951\n",
      "Eval -> relevance=0.5330, utilization=0.4406, adherence=0.9020, completeness=0.3665\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "'relevance_rate':   [],  \n",
    "'utilization_rate': [],\n",
    "'adherence_rate':   [],\n",
    "'completeness':     []\n",
    "}\n",
    "all_logits = {\n",
    "    'logits_relevance':   [],\n",
    "    'logits_utilization': [],\n",
    "    'logits_adherence':   []\n",
    "}\n",
    "all_labels = {\n",
    "    'labels_relevance':   [],\n",
    "    'labels_utilization': [],\n",
    "    'labels_adherence':   []\n",
    "}\n",
    "all_masks = {\n",
    "    'context_mask':  [],\n",
    "    'response_mask': []\n",
    "}\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in tqdm(val_loader):\n",
    "        inputs = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
    "        labels = {k: v for k,v in batch.items() if k.startswith('labels_')}\n",
    "        masks  = {k: v for k,v in batch.items() if k.endswith('_mask')}\n",
    "        out = model(**{k:inputs[k] for k in inputs})\n",
    "        batch_metrics = compute_trace_metrics_inference(out, masks)\n",
    "#             m = compute_batch_metrics(out, labels, masks)\n",
    "        for name, values in batch_metrics.items():\n",
    "            metrics[name].extend(values.cpu().tolist())\n",
    "#             sums += np.array(m)\n",
    "        for k in all_logits:\n",
    "#                 print(out[k].shape)\n",
    "            all_logits[k].append(out[k].cpu())\n",
    "        for k in all_labels:\n",
    "            all_labels[k].append(labels[k].cpu())\n",
    "        for k in all_masks:\n",
    "            all_masks[k].append(masks[k].cpu())\n",
    "    big_logits = { k: torch.cat(v, dim=0) for k, v in all_logits.items() }\n",
    "    big_labels = { k: torch.cat(v, dim=0) for k, v in all_labels.items() }\n",
    "    big_masks  = { k: torch.cat(v, dim=0) for k, v in all_masks.items() }\n",
    "    class_metrics = compute_classification_metrics(big_logits, big_labels, big_masks)\n",
    "    print(\"\\nClassification metrics on full validation set:\")\n",
    "    for name, val in class_metrics.items():\n",
    "        print(f\"  {name}: {val:.4f}\")\n",
    "#     avg = sums/count\n",
    "    print(f\"Eval -> relevance={np.mean(metrics['relevance_rate']):.4f}, utilization={np.mean(metrics['utilization_rate']):.4f}, adherence={np.mean(metrics['adherence_rate']):.4f}, completeness={np.mean(metrics['completeness']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6994294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коэффициент корреляции Пирсона: 0.45373879733665534\n",
      "p-value: 6.445192852460025e-184\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "r, p_value = stats.pearsonr(combined['validation']['relevance_score'], metrics['relevance_rate'])\n",
    "print(\"Коэффициент корреляции Пирсона:\", r)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80f8fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коэффициент корреляции Пирсона: 0.552252854067706\n",
      "p-value: 4.7870300467656045e-289\n"
     ]
    }
   ],
   "source": [
    "r, p_value = stats.pearsonr(combined['validation']['utilization_score'], metrics['utilization_rate'])\n",
    "print(\"Коэффициент корреляции Пирсона:\", r)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1aa24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
