{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a700e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from prompt_test.utils import send_chat_completion, send_async_requests\n",
    "from prompt_test.data import has_sentences, load_full_ragbench\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599a0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../tmp/emanual.test.pkl', 'rb') as f:\n",
    "#     t = pickle.load(f)\n",
    "# t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2ed852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msmarco': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "         num_rows: 1870\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "         num_rows: 423\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "         num_rows: 397\n",
       "     })\n",
       " }),\n",
       " 'pubmedqa': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "         num_rows: 19600\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "         num_rows: 2450\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
       "         num_rows: 2450\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragbench = load_full_ragbench()\n",
    "ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b93000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "from openai import AsyncOpenAI\n",
    "client = AsyncOpenAI(api_key='874c364705747e7ab314ceba89c2029c9a72ab2154664c470eb4ce18c2f0acb0', base_url= \"http://10.36.60.52:1234/v1\")\n",
    "model_id = 'qwen2.5-72b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await send_async_requests([[\n",
    "                {\"role\": \"system\", \"content\": 'Ты полезный бот'},\n",
    "                {\"role\": \"user\", \"content\": 'напиши стих'}\n",
    "            ]]*3, model=model_id, client=client,max_tokens=2048)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac55377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(item, prompt=\"Ты — профессиональный переводчик с английского на русский. Переводи точно и понятно.\"):\n",
    "    documents = \"\"\n",
    "    for doc in item['documents_sentences']:\n",
    "        for sentence in doc:\n",
    "            documents += \" \" + \" \".join(sentence)\n",
    "        documents += \"\\n\"\n",
    "    text = item['question'] + documents + item['response']\n",
    "#     print(text)\n",
    "    return [\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "\n",
    "def under_token_limit(item, prompt, tokenizer, max_tokens=8000):\n",
    "    messages = make_messages(item)\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # shape == (1, seq_len)\n",
    "    seq_len = encoded.shape[1]\n",
    "    return seq_len <= max_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e79c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "ragbench_new = {}\n",
    "\n",
    "for name, dataset in ragbench.items():\n",
    "    ragbench_new[name] = {}  \n",
    "    for subset in ['train', 'validation', 'test']:\n",
    "        if subset in dataset:\n",
    "            print(f'Датасет {name}.{subset}: {len(dataset[subset])} элементов')\n",
    "\n",
    "            filtered = dataset[subset].filter(has_sentences)\n",
    "            print(f'Датасет {name}.{subset} после фильтрации has_sentences: {len(filtered)} элементов')\n",
    "\n",
    "            filtered = filtered.filter(lambda x: under_token_limit(x, None, tokenizer, max_tokens=8000))\n",
    "            print(f'Датасет {name}.{subset} после фильтрации по max_tokens: {len(filtered)} элементов')\n",
    "#             filtered = filtered.shuffle(seed=42).select(range(16))\n",
    "            ragbench_new[name][subset] = filtered\n",
    "for ds_name, splits in ragbench_new.items():\n",
    "    ragbench_new[ds_name] = DatasetDict(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834215d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragbench_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_chat_completion_batch(batch_messages, model, client,\n",
    "                                     max_tokens=256, temperature=0.7, top_p=0.9, top_k=50):\n",
    "    extra_body = {\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    try:\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=batch_messages,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=42,\n",
    "            extra_body=extra_body\n",
    "        )\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            completion = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=batch_messages,\n",
    "                max_tokens=max_tokens,\n",
    "                seed=42,\n",
    "                extra_body=extra_body\n",
    "            )\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                completion = await client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=batch_messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    seed=42,\n",
    "                    extra_body=extra_body\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "    return [choice.message.content.strip() for choice in completion.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9291f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def translate_texts_in_batches(texts, model, client, max_tokens=256, temperature=0.7, top_p=0.9, top_k=50):\n",
    "    all_translations = []\n",
    "\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch_texts = texts[i:i + BATCH_SIZE]\n",
    "\n",
    "        batch_messages = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"Ты — профессиональный переводчик с английского на русский. Переводи точно и понятно. Рассуждения и тд не нужны, лишь перевод.\"},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "            for text in batch_texts\n",
    "        ]\n",
    "#         print(batch_messages)\n",
    "        batch_translations = await send_async_requests(\n",
    "            batch_messages,\n",
    "            model,\n",
    "            client,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k\n",
    "        )\n",
    "#         print(batch_translations)\n",
    "        all_translations.extend(batch_translations)\n",
    "\n",
    "    return all_translations\n",
    "\n",
    "def flatten_documents_sentences(documents_sentences):\n",
    "    texts = []\n",
    "    for doc_sentences in documents_sentences:\n",
    "        for sentence in doc_sentences:\n",
    "            # sentence пример: ['0a', 'Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 ...']\n",
    "            texts.append(sentence[1])\n",
    "    return texts\n",
    "\n",
    "def rebuild_documents_sentences_with_translations(documents_sentences, translations):\n",
    "    new_documents_sentences = []\n",
    "    idx = 0\n",
    "    for doc_sentences in documents_sentences:\n",
    "        new_doc = []\n",
    "        for sentence in doc_sentences:\n",
    "            number = sentence[0]\n",
    "            translated_text = translations[idx]\n",
    "            idx += 1\n",
    "            new_doc.append([number, translated_text])\n",
    "        new_documents_sentences.append(new_doc)\n",
    "    return new_documents_sentences\n",
    "\n",
    "async def translate_all_datasets(ragbench, text_fields, new_column_names, model, client):\n",
    "\n",
    "    updated_ragbench = {}\n",
    "\n",
    "    for dataset_name, dataset_splits in ragbench.items():\n",
    "        print(f\"Обрабатываем датасет: {dataset_name}\")\n",
    "        updated_ragbench[dataset_name] = {}\n",
    "\n",
    "        for split_name, split_dataset in dataset_splits.items():\n",
    "            print(f\"  Сплит: {split_name} — элементов: {len(split_dataset)}\")\n",
    "            questions = [item['question'] for item in split_dataset]\n",
    "            responses = [item['response'] for item in split_dataset]\n",
    "\n",
    "            all_docs_texts = []\n",
    "            docs_indices = []  \n",
    "            for i, item in enumerate(split_dataset):\n",
    "                docs_texts = flatten_documents_sentences(item['documents_sentences'])\n",
    "                all_docs_texts.extend(docs_texts)\n",
    "                docs_indices.append((i, len(docs_texts)))\n",
    "\n",
    "            print(\"  Перевод вопросов...\")\n",
    "            questions_translated = await translate_texts_in_batches(questions, model, client)\n",
    "            print(\"  Перевод ответов...\")\n",
    "            responses_translated = await translate_texts_in_batches(responses, model, client)\n",
    "            print(\"  Перевод предложений из документов...\")\n",
    "            docs_translated = await translate_texts_in_batches(all_docs_texts, model, client)\n",
    "\n",
    "            new_documents_sentences = [[] for _ in range(len(split_dataset))]\n",
    "            idx = 0\n",
    "            for elem_idx, length in docs_indices:\n",
    "                translated_sents = docs_translated[idx: idx + length]\n",
    "                idx += length\n",
    "                new_documents_sentences[elem_idx] = rebuild_documents_sentences_with_translations(\n",
    "                    split_dataset[elem_idx]['documents_sentences'],\n",
    "                    translated_sents\n",
    "                )\n",
    "\n",
    "            new_items = []\n",
    "            for i, item in enumerate(split_dataset):\n",
    "                new_item = dict(item)\n",
    "                new_item[new_column_names[0]] = questions_translated[i]\n",
    "                new_item[new_column_names[1]] = responses_translated[i]\n",
    "                new_item[new_column_names[2]] = new_documents_sentences[i]\n",
    "                new_items.append(new_item)\n",
    "\n",
    "            from datasets import Dataset\n",
    "            updated_split = Dataset.from_list(new_items)\n",
    "            updated_ragbench[dataset_name][split_name] = updated_split\n",
    "\n",
    "    return updated_ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c894c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "def batched(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "async def translate_all_datasets_chunked(ragbench, text_fields, new_column_names, model, client, ckpt_dir=\"checkpoints\"):\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    updated_ragbench = {}\n",
    "\n",
    "    for dataset_name, dataset_splits in ragbench.items():\n",
    "        print(f\"Обрабатываем датасет: {dataset_name}\")\n",
    "        updated_ragbench[dataset_name] = {}\n",
    "\n",
    "        for split_name, split_dataset in dataset_splits.items():\n",
    "            print(f\"  Сплит: {split_name} — элементов: {len(split_dataset)}\")\n",
    "\n",
    "            ckpt_path = os.path.join(ckpt_dir, f\"{dataset_name}_{split_name}.jsonl\")\n",
    "            processed_items = []\n",
    "            if os.path.exists(ckpt_path):\n",
    "                with open(ckpt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        processed_items.append(json.loads(line))\n",
    "                print(f\"    Найден checkpoint: {len(processed_items)} элементов уже обработано\")\n",
    "            else:\n",
    "                print(\"    Нет checkpoint, начинаем с начала\")\n",
    "\n",
    "            N_done = len(processed_items)\n",
    "\n",
    "            if N_done == len(split_dataset):\n",
    "                print(\"    Все элементы уже обработаны, загружаем из checkpoint\")\n",
    "                ds = Dataset.from_list(processed_items)\n",
    "                updated_ragbench[dataset_name][split_name] = ds\n",
    "                continue\n",
    "\n",
    "            with open(ckpt_path, \"a\", encoding=\"utf-8\") as f:\n",
    "\n",
    "                for batch_start in range(N_done, len(split_dataset), BATCH_SIZE):\n",
    "                    batch_end = min(batch_start + BATCH_SIZE, len(split_dataset))\n",
    "                    batch = split_dataset[batch_start:batch_end]\n",
    "#                     print(batch)\n",
    "                    questions =  batch['question'] #[item['question'] for item in batch]\n",
    "#                     print()\n",
    "#                     questions_translated = await send_async_requests(\n",
    "#                         questions,\n",
    "#                         model,\n",
    "#                         client,\n",
    "#                         max_tokens=2048,\n",
    "#                         temperature=0.7,\n",
    "#                         top_p=0.9,\n",
    "#                         top_k=50\n",
    "#                     )\n",
    "                    questions_translated = await translate_texts_in_batches(questions, model, client)\n",
    "#                     print(questions_translated)\n",
    "                    responses = batch['response'] #[item['response'] for item in batch]\n",
    "#                     responses_translated = await send_async_requests(\n",
    "#                         responses,\n",
    "#                         model,\n",
    "#                         client,\n",
    "#                         max_tokens=2048,\n",
    "#                         temperature=0.7,\n",
    "#                         top_p=0.9,\n",
    "#                         top_k=50\n",
    "#                     )\n",
    "        \n",
    "                    responses_translated = await translate_texts_in_batches(responses, model, client)\n",
    "                    all_docs_texts = []\n",
    "                    docs_indices = []\n",
    "\n",
    "                    batch_size = len(batch['id'])\n",
    "                    for i in range(batch_size):\n",
    "                        docs_texts = flatten_documents_sentences(batch['documents_sentences'][i])\n",
    "                        all_docs_texts.extend(docs_texts)\n",
    "                        docs_indices.append((i, len(docs_texts)))\n",
    "#                     print(docs_indices, all_docs_texts)\n",
    "                    docs_translated = await translate_texts_in_batches(all_docs_texts, model, client)\n",
    "\n",
    "                    new_documents_sentences = [[] for _ in range(len(batch['id']))]\n",
    "                    idx = 0\n",
    "                    for elem_idx, length in docs_indices:\n",
    "                        translated_sents = docs_translated[idx: idx + length]\n",
    "                        idx += length\n",
    "                        new_documents_sentences[elem_idx] = rebuild_documents_sentences_with_translations(\n",
    "                            batch['documents_sentences'][elem_idx],\n",
    "                            translated_sents\n",
    "                        )\n",
    "\n",
    "                    for i in range(len(batch['id'])):\n",
    "                        new_item = {key: batch[key][i] for key in batch.keys()}\n",
    "\n",
    "                        new_item[new_column_names[0]] = questions_translated[i]\n",
    "                        new_item[new_column_names[1]] = responses_translated[i]\n",
    "                        new_item[new_column_names[2]] = new_documents_sentences[i]\n",
    "\n",
    "                        f.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "\n",
    "#                         print(f\"Сохранён элемент {batch_start + i + 1}/{len(split_dataset)}\")\n",
    "#                     break\n",
    "            ds = load_dataset(\n",
    "                \"json\",\n",
    "                data_files={split_name: ckpt_path},\n",
    "                split=split_name\n",
    "            )\n",
    "            updated_ragbench[dataset_name][split_name] = ds\n",
    "\n",
    "    return updated_ragbench\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef140773",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    translated_ragbench = await translate_all_datasets_chunked(\n",
    "        ragbench_new,\n",
    "        text_fields=['question', 'response', 'documents_sentences'],\n",
    "        new_column_names=['question_ru', 'response_ru', 'documents_sentences_ru'],\n",
    "        model=model_id,\n",
    "        client=client\n",
    "    )\n",
    "    return translated_ragbench\n",
    "\n",
    "\n",
    "translated_ragbench = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4478e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampled(bench, idx, dataset, fold, ru_field, field):\n",
    "    return bench[dataset][fold][idx][ru_field], bench[dataset][fold][idx][field]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled(translated_ragbench,8, 'hagrid', 'train', 'response_ru', 'response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183359d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('../hagrid_ru.pkl', 'wb') as f:\n",
    "#     pickle.dump(translated_ragbench, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6581a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delucionqa': {'train': Dataset({\n",
       "      features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score', 'question_ru', 'response_ru', 'documents_sentences_ru'],\n",
       "      num_rows: 1458\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score', 'question_ru', 'response_ru', 'documents_sentences_ru'],\n",
       "      num_rows: 182\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score', 'question_ru', 'response_ru', 'documents_sentences_ru'],\n",
       "      num_rows: 184\n",
       "  })}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../delucionqa_ru.pkl', 'rb') as f:\n",
    "    translated_ragbench = pickle.load(f)\n",
    "translated_ragbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2782eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
